# Формирование и исследование выборки научных документов из цифровой библиотеки ACM (Formation and research of sample of scientific documents from ACM Digital Library)

---

Данная работа выполнялась мной, Козлов П.А. и Толчеев В.О. выступали в роли научных консультантов.

Доклад (ещё не опубликован) был представлен на XXXI Международной научно-технической конференции «Современные технологии в задачах управления, автоматики и обработки информации»

---

# Аннотация

В докладе рассматривается задача формирования выборки англоязычных научных статей из цифровой библиотеки ACM для решения задачи бинарной классификации. Дается описание выборки, алгоритмов предварительной обработки текстовых данных и методов классификации. Приводятся показатели качества бинарной классификации на исследуемой выборке.

# Доклад

При анализе современных научных статей возникает проблема их систематизации, вследствие чего по-является необходимость их разделения по тематикам (классам). Данную проблему можно формализовать в виде задачи бинарной или многоклассовой классификации, решение которой даёт возможность автомати-ческого определения тематик статей, то есть их автоматическую разметку.

Автоматическая обработка и рубрикация текстов особенно актуальны в научной области, в которой ис-следователям необходимо просматривать большие объемы информации в области своей специализации. В нашей работе предполагается, что информационная потребность исследователя заключается в отслежива-нии тематики “Интеллектуальный анализ данных” (ИАД) и необходимо построить бинарный классифика-тор, способный разделять документы на класс “ИАД” и класс “НЕ ИАД”, включающий все остальные тема-тики, которые встречаются в выборке.

Ранее на основе российской цифровой библиотеки eLibrary нам удалось сформировать выборку неболь-шого размера из-за ограниченного количества релевантных статей по ИАД. В связи с этим было принято решение составить новую выборку “ИАД”-“НЕ ИАД” из англоязычной цифровой библиотеки “ACM Digital Library”, обладающей большим числом научных документов в области ИАД. Отметим, что документальный массив, полученный из ACM, может использоваться как для обучения англоязычного классификатора, так и русскоязычного после осуществления машинного перевода. Учитывая, что в ACM бесплатный доступ предоставляется только к библиографическим описаниям документов, далее для анализа используются только заголовки и аннотации статей.

При формировании класса “ИАД” применялся поисковый запрос по ключевым словам: “Text Mining, Clustering, Classification, Machine Learning, Data Mining”, а для “НЕ ИАД” в запрос вошли: “Hardware Archi-tecture”, “Software Engineering”, “Fuzzy Logic”, “Systems and Control”, “Databases”, “Computer Security”. В ACM имеется ряд дополнительных возможностей, которые мы использовали при составлении выборки: ограничение даты выхода статей равное 5 годам, сортировка по релевантности, использование логических операторов в поисковом запросе.

По сформированным запросам было получено 4000 документов, из которых, с помощью генератора равномерно распределённых случайных чисел, была составлена выборка для исследований. Она включает 1500 публикаций по “ИАД” и 1500 по “НЕ ИАД”. Для использования программных реализаций алгоритмов машинного обучения к меткам классов применяется бинарное кодирование, то есть 0 – “ИАД”, 1 – “НЕ ИАД”.

На этапе предварительной обработки текстовых данных из выборки удалены стоп-слова вместе со зна-ками пунктуации, одночастотные слова, а также осуществлена лемматизация. Далее применяется модель «мешок слов» и проводится tfc-взвешивание [1] для определения весов информативных терминов. 

Для проведения бинарной классификации в данной работе используются хорошо изученные подходы, основанные на различных принципах принятия решений: метод k-ближайших соседей, логистическая ре-грессия, случайный лес. Перед обучением моделей исходная выборка была разделена случайным образом с сохранением пропорций классов на обучающую и тестовую в соотношении 70/30 % для проверки качества полученного решения. Улучшение результатов обучения данных моделей производилось с помощью поис-ка гиперпараметров по заданной сетке с помощью алгоритма перекрёстной проверки на обучающей выбор-ке. При этом обучающая выборка разбивалась на 5 подвыборок, из них 4 использовались для обучения, а одна для расчета показателей качества, которые включают accuracy (правильность – доля правильных ре-шений), recall (полнота), precision (точность) и F1-меру.

После обучения классификаторов на тестовой выборке получены следующие результаты: accuracy – 0.914, precision – 0.911, recall – 0.935, F1 – 0.923 для случайного леса, accuracy – 0.906, precision – 0.936, recall – 0.888, F1 – 0.912 для логистической регрессии, accuracy – 0.898, precision – 0.899, recall – 0.917, F1 – 0.908 для метода k-ближайших соседей. Таким образом, наилучшая бинарная классификация на сформированной выборке обеспечивается при использовании алгоритма случайный лес.

# Список литературы

1.	Маннинг К., Рагхаван П., Шютце Х. Введение в информационный поиск. – М.:   Вильямс, 2014 –528 с.

# Описание ноутбуков

parser_iad.ipynb - парсинг статей по тематике ИАД.

parser_no_iad.ipynb - парсинг статей по тематике НЕ_ИАД.

text_analysis.ipynb - анализ статей полученных в результате парсинга ACM.

text_analysis_ru.ipynb - анализ переведённых статей в результате парсинга ACM.

# Результаты

1. [Выгрузка данных из цифровых библиотек](#load)

2. [Фильтрация данных](#filter)

3. [Подходы к формированию выборок из цифровых библиотек](#form)

4. [Выбор необходимых признаков описания статей](#choice)

5. [Реализация программы для выгрузки данных из цифровых библиотек](#prog)

6. [Предварительная обработка данных](#preproc)

7. [Визуализация данных и распределение классов в количественном соотношении с их группировкой в признаковом пространстве](#vis)

8. [Кластеризация данных и сравнение полученных меток с исходными](#clust)

9. [Классификация данных](#class)

10. [Вывод](#conc)


## **Выгрузка данных из цифровых библиотек** <a name="load"></a>

В результате анализа ряда иностранных цифровых библиотек с точки зрения выгрузки из них данных была выбрана библиотека “ACM Digital Library”, в дальнейшем “ACM”, у которой нет разработанного API, но она имеет большое количество статей, расширенный поиск, много параметров URL и её удобно парсить.


## **Фильтрация данных** <a name="filter"></a>

Библиотека “ACM”, как упоминалось ранее имеет расширенный поиск, который позволяет использовать логические операторы при формировании запроса и выбирать временной интервал дат выхода статей.


## **Подходы к формированию выборок из цифровых библиотек** <a name="form"></a>

Сбор данных организовывался для решения задачи бинарной классификации научных статей по разделам ИАД и НЕ ИАД. Для формирования псевдогенеральной совокупности класса ИАД использовался запрос “Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining”, а для класса НЕ ИАД были отобраны экспертным путём следующие тематики: аппаратная архитектура (Hardware Architecture), программная инженерия (Software Engineering), нечёткая логика (Fuzzy Logic), системы и средства управления (Systems and Control), базы данных (Databases), компьютерная безопасность (Computer Security). При этом использовались ограничение даты выхода статьи равное 5 годам и сортировка по релевантности, что должно обеспечить хорошую пертинентность.


## **Выбор необходимых признаков описания статей** <a name="choice"></a>

Для описания статей были выбраны следующие признаки: заголовок, ссылка, авторы, дата выхода, аннотация, источник. Такие признаки достаточно полно описывают статьи для решения поставленной задачи.


## **Реализация программы для выгрузки данных из цифровых библиотек** <a name="prog"></a>

Программа была реализована на языке программирования “Python”, в результате выполнения которой был сформирован набор из 2000 статей по ИАД и 2400 статей по НЕ ИАД, то есть по 600 статей каждой тематики НЕ ИАД.


## **Предварительная обработка данных** <a name="preproc"></a>

Из имеющейся псевдогенеральной совокупности с помощью генератора случайных чисел была сформирована выборка из 1700 статей по ИАД и 1700 статей по НЕ ИАД, при чём в результате такого подхода распределение тематик класса НЕ ИАД осталось достаточно сбалансированным: аппаратная архитектура – 18.1%, программная инженерия – 17.9%, нечёткая логика – 16.5%, системы и средства управления – 16.2%, базы данных – 16.1%, компьютерная безопасность – 15.2%. В качестве предварительной обработки использовались лемматизация, удаление стоп-слов и знаков пунктуации, бинарное кодирование меток классов (0 – ИАД, 1 – НЕ ИАД), отсечение одночастотных слов, после чего производилось tfc-взвешивание.

Результаты обучения Наивного Байеса с помощью K-fold cross validation представлены на рисунке 1.

![](pic/image1.png)

Рис. 1. График точности и количества слов к частоте отсечения.

По рисунку 1 было определено, что при частоте отсечения 11, разброс результатов мал, по сравнению с результатами для ближайших частот, при этом в словаре ещё достаточно много слов (3772), поэтому эта частота использовалась в дальнейшем для классификации.


## **Визуализация данных и распределение классов в количественном соотношении с их группировкой в признаковом пространстве** <a name="vis"></a>

Для анализа слов составляющих классы ИАД и НЕ ИАД были составлены облака слов по частоте встречаемости в статье.

![](pic/image2.png)

Рис. 2. Облако слов для ИАД.

![](pic/image3.png)

Рис. 3. Облако слов для НЕ ИАД.

![](pic/image4.png)

Рис. 4. Общее облако слов.

По данным рисункам можно сделать вывод, что у классов есть общий набор слов, однако наблюдаются и отдельные слова, характерные только для соответствующих классов.

Для более подробного анализа слов составляющих классы ИАД и НЕ ИАД были составлены облака слов со взвешиванием tfc.

![](pic/image8.png)

Рис. 5. Облако слов для ИАД.

![](pic/image9.png)

Рис. 6. Облако слов для НЕ ИАД.

![](pic/image10.png)

Рис. 7. Общее облако слов.

Для анализа распределения классов в пространстве было проведено понижение размерности методом главных компонент (МГК) и стохастического вложения соседей с t-распределением (T-SNE).

![](pic/image5.png)

Рис. 8. Двумерный график МГК.

![](pic/image6.png)

Рис. 9. Трёхмерный график МГК.

![](pic/image7.png)

Рис. 10. Трёхмерный график МГК.

По результатам снижения размерности наблюдается сильное пересечение двух классов и наличие выбросов, однако имеется явно выраженная область класса ИАД, что позволяет сделать предварительный вывод о возможной разделимости классов.


## **Кластеризация данных и сравнение полученных меток с исходными** <a name="clust"></a>

![](pic/image11.png)

Рис. 11. Кластеризация методом K-Средних.

Количество совпадений меток: ИАД – 1307, НЕ ИАД – 1527, отличия – 502. Ошибки обусловлены алгоритмом работы K-Средних, а также сильным пересечением классов.

## **Классификация данных** <a name="class"></a>

В качестве моделей для обучения были выбраны: случайный лес, логистическая регрессия, k-ближайших соседей. Данный выбор обусловлен тем, что “лес” представляет собой ансамбль моделей, лог. регрессия позволяет выявлять нелинейные зависимости, а k-ближайших соседей является метрическим алгоритмом.

Предварительно исходная выборка была разделена на обучающую и тестовую в соотношении 70/30 %.

Для улучшения результатов обучения моделей произведён поиск гиперпараметров по заданной сетке, в результате которого были выбраны следующие: 
-	случайный лес
    - количество деревьев 115
    - максимальная глубина – 45
    - минимальное количество элементов в листе – 3
    - минимальное количество элементов для разделения – 11
- логистическая регрессия
    - L2 регуляризация
    - коэффициент регуляризации – 0.1
    - алгоритм оптимизации – квазиньютоновский метод
-	k-ближайших соседей
    - число соседей – 201
    - метрика - косинусное расстояние
    - взвешивание – расстояние

Результаты обучения на обучающей выборке с помощью K-fold cross validation с количеством фолдов равным 5 представлены в таблице.

|             |                   Случайный лес           |           Логистическая регрессия         |            K-ближайших соседей            |
| :----------:|:-----------------------------------------:|:-----------------------------------------:|:-----------------------------------------:|
|  Accuracy   | 0.939<br>0.954<br>0.931<br>0.941<br>0.943 | 0.945<br>0.956<br>0.916<br>0.952<br>0.935 | 0.914<br>0.941<br>0.927<br>0.935<br>0.922 |
|  Precision  | 0.973<br>0.991<br>0.953<br>0.965<br>0.958 | 0.946<br>0.959<br>0.904<br>0.951<br>0.931 | 0.943<br>0.978<br>0.964<br>0.953<br>0.929 |
|  Recall     | 0.905<br>0.917<br>0.91<br>0.918<br>0.93   | 0.946<br>0.955<br>0.934<br>0.955<br>0.942 | 0.884<br>0.905<br>0.888<br>0.918<br>0.918 |
|  F1_micro   | 0.939<br>0.954<br>0.931<br>0.941<br>0.943 | 0.943<br>0.956<br>0.924<br>0.954<br>0.933 | 0.914<br>0.94<br>0.926<br>0.935<br>0.922  |

Сводка показателей качества случайного леса на тестовой выборке:

|               | precision | recall | f1-score | support  |
| :------------:|:---------:|:------:|:--------:|:--------:|
|  0            |    0.91   |  0.95  |   0.93   |    533   |
|  1            |    0.95   |  0.90  |   0.92   |    487   |
|  accuracy     |           |        |   0.93   |    1020  |
|  macro avg    |    0.93   |  0.93  |   0.93   |    1020  |
|  weighted avg |    0.93   |  0.93  |   0.93   |    1020  |

Сводка показателей качества логистической регрессии на тестовой выборке:

|               | precision | recall | f1-score | support  |
| :------------:|:---------:|:------:|:--------:|:--------:|
|  0            |    0.94   |  0.91  |   0.92   |    533   |
|  1            |    0.91   |  0.93  |   0.92   |    487   |
|  accuracy     |           |        |   0.92   |    1020  |
|  macro avg    |    0.92   |  0.92  |   0.92   |    1020  |
|  weighted avg |    0.92   |  0.92  |   0.92   |    1020  |

Сводка показателей качества K-ближайших соседей на тестовой выборке:

|               | precision | recall | f1-score | support  |
| :------------:|:---------:|:------:|:--------:|:--------:|
|  0            |    0.89   |  0.94  |   0.92   |    533   |
|  1            |    0.93   |  0.87  |   0.90   |    487   |
|  accuracy     |           |        |   0.91   |    1020  |
|  macro avg    |    0.91   |  0.91  |   0.91   |    1020  |
|  weighted avg |    0.91   |  0.91  |   0.91   |    1020  |

![](pic/image12.png)

Рис. 12. ROC кривая.


## **Вывод** <a name="conc"></a>

По полученным результатам классификации можно сделать вывод, о том, что сформированная выборка позволяет решать поставленную задачу бинарной классификации с хорошей точностью, при чём лучшей моделью оказался случайный лес, так как он даёт в среднем наивысшие показатели качества, и при этом они устойчивы.

---

(c) Ларчев В.И., Козлов П.А., Толчеев В.О., 2022.